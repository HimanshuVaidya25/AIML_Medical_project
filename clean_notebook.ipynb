{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0lLOBsAuflg",
    "outputId": "4ba17026-3406-4c07-ca41-51047a8857fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hPython: 3.12.11\n",
      "Torch: 2.8.0+cu126\n",
      "CUDA available: True\n",
      "GPU 0: Tesla T4 (UUID: GPU-e29d2dec-5519-4b5c-9ab7-0dbb29dccb92)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets peft accelerate bitsandbytes safetensors huggingface_hub sentencepiece\n",
    "\n",
    "import torch, sys\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "!nvidia-smi -L || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "9b_-5LwiuvRf"
   },
   "outputs": [],
   "source": [
    "import os, torch\n",
    "from pathlib import Path\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "e35d72474b0b4748ae686a9989177a74",
      "d580ad5d846847bab5a428eb722f8e1f",
      "47de4874859f406382cf7a94c429e281",
      "db469edc09924080ae8de282c6df7b2e",
      "1110d4d8dda446ecb92cbda2d313d7a8",
      "22ca433abdf241948f1f3d871ef72753",
      "8423ade415fa42f19a7e68be30175605",
      "6a03a8ce3ea649359c44a8931f65e129",
      "31aed6e6388d42c1875c18266e0a4a28",
      "6ffbb249879a48b08bed66e1106bcdbe",
      "5edee29c17fb4ef69f6e48ff7784ead0",
      "d1df855887f841669ab266f47a65e2bf",
      "8bb57e78c68b4b5aacabb5a4f2b9e434",
      "548be4eab65e4ca08dc171b0f43062c3",
      "a4235b69b27b4c089325afb5eab8cf49",
      "b2a37193509b458199721dac462ebf12",
      "e818c46d5cf447b29a7251b249ea767b",
      "699fd835a3214052838d5f40fdad0223",
      "994dc8a20dda41228c0f1a86d7018b19",
      "3eb0394132d64f4a8441c623737f8024"
     ]
    },
    "id": "HEuc6-cPu18g",
    "outputId": "e3446165-c3a3-4060-9df8-88682d33fb72"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35d72474b0b4748ae686a9989177a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()\n",
    "MODEL_NAME = \"tiiuae/falcon-rw-1b\"\n",
    "OUTPUT_DIR = \"lora_adapter\"\n",
    "RESULTS_DIR = \"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "j8zhT0Xnu7iJ"
   },
   "outputs": [],
   "source": [
    "train_samples = 1000\n",
    "eval_samples = 100\n",
    "per_device_train_batch_size = 1\n",
    "per_device_eval_batch_size = 1\n",
    "num_train_epochs = 1\n",
    "learning_rate = 3e-4\n",
    "max_length = 512\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wZ5S8dj6u_BR",
    "outputId": "a84e8961-fc10-48b2-cfc9-dc87301b7adb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7c94e5707870>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zt24UGv9vBOd",
    "outputId": "88a29e00-2f5e-4f92-cf15-cfdf07453467"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config done! Model: tiiuae/falcon-rw-1b\n"
     ]
    }
   ],
   "source": [
    "print(\"Config done! Model:\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "25rtDKZNvEw5"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "H2jDTH5CvH0E"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"lavita/AlpaCare-MedInstruct-52k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12uOHn9WvKGq",
    "outputId": "afe63f24-596b-46f4-abc1-a5baac9e59a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['output', 'input', 'instruction'],\n",
      "        num_rows: 52002\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lc0xn86EvNkX",
    "outputId": "450221e1-63a4-45da-82d0-ab0c41fced54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample 1 ---\n",
      "Instruction: Explain why a mass in the lung could cause shortness of breath.\n",
      "Response: A mass in the lung could cause shortness of breath due to several reasons. First, the mass can physically obstruct the air passages, causing difficulty in airflow and leading to breathing difficulties ...\n",
      "\n",
      "--- Sample 2 ---\n",
      "Instruction: Write a brief reflection on what you learned from today's lecture on diabetes.\n",
      "Response: Today's lecture on diabetes provided a comprehensive overview of the condition, its causes, risk factors, and management strategies. \n",
      "\n",
      "I learned that diabetes is a chronic disease characterized by hig ...\n",
      "\n",
      "--- Sample 3 ---\n",
      "Instruction: Provide a condensed summary about this prescription medication.\n",
      "Response: Lexapro (Escitalopram) is a prescription medication that belongs to a class of drugs called selective serotonin reuptake inhibitors (SSRIs). It is primarily used for the treatment of depression and ge ...\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(\"Instruction:\", dataset[\"train\"][i][\"instruction\"])\n",
    "    print(\"Response:\", dataset[\"train\"][i][\"output\"][:200], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "zxOyv2xRvQvh"
   },
   "outputs": [],
   "source": [
    "def format_example(example):\n",
    "    instr = example.get(\"instruction\", \"\")\n",
    "    resp = example.get(\"output\", \"\")\n",
    "    instr = instr.strip() if isinstance(instr, str) else \"\"\n",
    "    resp = resp.strip() if isinstance(resp, str) else \"\"\n",
    "    text = f\"### Instruction:\\n{instr}\\n\\n### Response:\\n{resp}\\n\"\n",
    "    return {\"text\": text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "pmLqIyVDvUwp"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(format_example, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W6TnItCvvYNS",
    "outputId": "b52a587d-2a6a-4356-d96c-5ece5c15d8f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Explain why a mass in the lung could cause shortness of breath.\n",
      "\n",
      "### Response:\n",
      "A mass in the lung could cause shortness of breath due to several reasons. First, the mass can physically obstruct the air passages, causing difficulty in airflow and leading to breathing difficulties. Second, if the mass is cancerous or infected, it can cause inflammation and damage to lung tissue, reducing its functional capacity and compromising normal breathing. Additionally, a lung mass can compr\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "bjGdvUbnvbFw"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "M765Zy1hvhhf"
   },
   "outputs": [],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].shuffle(seed=42).select(range(min(1000, len(dataset[\"train\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afcoxVrXvjvN",
    "outputId": "b7d6b0a8-0205-423e-f47f-5696bbe70ced"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 900\n",
      "Validation samples: 100\n",
      "\n",
      "Example training text:\n",
      " ### Instruction:\n",
      "As a patient, ask questions about basic precautions to be taken during the flu season.\n",
      "\n",
      "### Response:\n",
      "To prevent catching the flu during the winter season, there are several basic precautions you can take:\n",
      "\n",
      "1. Get vaccinated: The flu vaccine is the most effective way to protect your\n"
     ]
    }
   ],
   "source": [
    "split_data = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "train_ds = split_data[\"train\"]\n",
    "val_ds = split_data[\"test\"]\n",
    "\n",
    "print(\"Train samples:\", len(train_ds))\n",
    "print(\"Validation samples:\", len(val_ds))\n",
    "print(\"\\nExample training text:\\n\", train_ds[0][\"text\"][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "7RQabS7_voQp"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "j6AwL5QrvsB8"
   },
   "outputs": [],
   "source": [
    "model_name = \"tiiuae/falcon-rw-1b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "tSl_D5qIv0hS"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"</s>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "w7nYq8YJv21C"
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ANW3NAKv5Jk",
    "outputId": "313b945b-b13d-42ba-f27c-52d0add8c79c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AjzBb68nv9Bj",
    "outputId": "b2f13f00-f2e1-420e-d918-bccdee744a70"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and prepared for training.\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(\"Model loaded and prepared for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "Qsbm8XNjws29"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w0EBBhPswz2B",
    "outputId": "fba7bdf4-beca-4d8e-b22f-ce396778cb3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,572,864 || all params: 1,313,103,872 || trainable%: 0.1198\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "USCKUN7Dw2q9"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    tokens = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "5JJ2AUBrw6_k"
   },
   "outputs": [],
   "source": [
    "tokenized_train = train_ds.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_val = val_ds.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "koYsN6lBxBu4"
   },
   "outputs": [],
   "source": [
    "tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_val.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_p_ipapBxGB_",
    "outputId": "9f28b6a0-0616-4fb3-d9a9-eca80c97bb59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete.\n",
      "Example token IDs: tensor([21017, 46486,    25,   198,  1722,   257,  5827,    11,  1265,  2683,\n",
      "          546,  4096, 31320,   284,   307,  2077,  1141,   262,  6562,  1622,\n",
      "           13,   198,   198, 21017, 18261,    25,   198,  2514,  2948, 16508])\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenization complete.\")\n",
    "print(\"Example token IDs:\", tokenized_train[0][\"input_ids\"][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6B8yfMphxL1-",
    "outputId": "54c9157c-e17d-46a7-b695-62b9f5bc6c50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting stable manual training for 1 epoch(s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:   0%|          | 0/900 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Epoch 1/1: 100%|██████████| 900/900 [13:23<00:00,  1.12it/s, loss=1.74]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Stable manual training complete! Model fine-tuned successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "train_loader = DataLoader(tokenized_train, batch_size=1, shuffle=True)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "epochs = 1\n",
    "print(f\" Starting stable manual training for {epochs} epoch(s)...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch, use_cache=False)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "print(\" Stable manual training complete! Model fine-tuned successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nw-0m1j4xeV4",
    "outputId": "80c855c6-6223-487d-d2db-957b0727d865"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving adapter to: lora_adapter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter and tokenizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "OUTPUT_DIR = \"lora_adapter\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Saving adapter to:\", OUTPUT_DIR)\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"Adapter and tokenizer saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CbK3mjCs6A7R",
    "outputId": "8aae6546-8ede-4325-8f31-d2698c8e4564"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model for inference\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 2048)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inference test\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "model_name = \"tiiuae/falcon-rw-1b\"\n",
    "\n",
    "print(\"Loading base model for inference\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "base_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZYI_Orv6FjH",
    "outputId": "67d55b16-e5fb-440f-dec0-faf9283a2ecb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model for inference\n"
     ]
    }
   ],
   "source": [
    "#inference test\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "model_name = \"tiiuae/falcon-rw-1b\"\n",
    "\n",
    "# Load tokenizer here to ensure it's defined\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"</s>\"})\n",
    "\n",
    "\n",
    "print(\"Loading base model for inference\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, \"lora_adapter\")\n",
    "model.eval()\n",
    "\n",
    "def generate_with_disclaimer(prompt, max_new_tokens=150):\n",
    "    input_text = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, use_cache=False)\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"### Response:\" in text:\n",
    "        text = text.split(\"### Response:\")[-1].strip()\n",
    "    disclaimer = (\n",
    "        \"\\n\\n---\\n *This response is for educational purposes only \"\n",
    "        \"and is NOT medical advice. Consult a qualified healthcare professional.*\"\n",
    "    )\n",
    "    return text + disclaimer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PK6HEc8i6Jp2",
    "outputId": "aa8b2228-be86-41d1-adfe-0e0eb6fbeca9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model test output:\n",
      "\n",
      "To treat a mild fever at home safely, follow these steps:\n",
      "\n",
      "1. Assess the patient's condition:\n",
      "- Check the temperature: Take the patient's temperature with a digital thermometer. It should be taken in the morning and after a period of rest.\n",
      "- Check for other symptoms: Check for other symptoms such as chills, headache, muscle aches, and sore throat.\n",
      "- Monitor the patient's condition: Monitor the patient's condition for any changes in symptoms or signs.\n",
      "2. Consider other causes of fever:\n",
      "- Check for other possible causes of fever such as infection, medication, or stress.\n",
      "- Consult a healthcare professional: Consult a healthcare professional if the patient's fever persists or worsens.\n",
      "\n",
      "---\n",
      " *This response is for educational purposes only and is NOT medical advice. Consult a qualified healthcare professional.*\n"
     ]
    }
   ],
   "source": [
    "print(\"Model test output:\\n\")\n",
    "print(generate_with_disclaimer(\"Explain how to treat a mild fever at home safely.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "iV4jHe4Y2KXj",
    "outputId": "f7d6e5e8-c365-4189-9802-6eed8ab21261"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter zipped successfully as lora_adapter.zip\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_d9bd9b76-f0db-448d-aaa9-33f485eb3f32\", \"lora_adapter.zip\", 391999738)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "shutil.make_archive(\"lora_adapter\", \"zip\", \"lora_adapter\")\n",
    "print(\"Adapter zipped successfully as lora_adapter.zip\")\n",
    "\n",
    "files.download(\"lora_adapter.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-00o0UOpaJpj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
